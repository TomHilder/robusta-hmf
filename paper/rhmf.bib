@article{aimejudicaelouermi2025,
  title = {A {{Fast Iterative Robust Principal Component Analysis Method}}},
  author = {Aime Judicael Ouermi, Timbwaoga and Li, Jixian and Johnson, Chris R.},
  year = 2025,
  month = jun,
  journal = {arXiv e-prints},
  pages = {arXiv:2506.16013},
  doi = {10.48550/arXiv.2506.16013},
  url = {https://ui.adsabs.harvard.edu/abs/2025arXiv250616013A/abstract},
  urldate = {2026-02-04},
  abstract = {Principal Component Analysis (PCA) is widely used for dimensionality reduction and data analysis. However, PCA results are adversely affected by outliers often observed in real-world data. Existing robust PCA methods are often computationally expensive or exhibit limited robustness. In this work, we introduce a Fast Iterative Robust (FIR) PCA method by efficiently estimating the inliers center location and covariance. Our approach leverages Incremental PCA (IPCA) to iteratively construct a subset of data points that ensures improved location and covariance estimation that effectively mitigates the influence of outliers on PCA projection. We demonstrate that our method achieves competitive accuracy and performance compared to existing robust location and covariance methods while offering improved robustness to outlier contamination. We utilize simulated and real-world datasets to evaluate and demonstrate the efficacy of our approach in identifying and preserving underlying data structures in the presence of contamination.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/DT6JQGH9/Aime Judicael Ouermi et al. - 2025 - A Fast Iterative Robust Principal Component Analysis Method.pdf}
}

@article{bailey2012,
  title = {Principal {{Component Analysis}} with {{Noisy}} and/or {{Missing Data}}},
  author = {Bailey, Stephen},
  year = 2012,
  month = sep,
  journal = {Publications of the Astronomical Society of the Pacific},
  volume = {124},
  number = {919},
  eprint = {1208.4122},
  primaryclass = {astro-ph},
  pages = {1015--1023},
  issn = {00046280, 15383873},
  doi = {10.1086/668105},
  url = {http://arxiv.org/abs/1208.4122},
  urldate = {2026-02-04},
  abstract = {We present a method for performing Principal Component Analysis (PCA) on noisy datasets with missing values. Estimates of the measurement error are used to weight the input data such that compared to classic PCA, the resulting eigenvectors are more sensitive to the true underlying signal variations rather than being pulled by heteroskedastic measurement noise. Missing data is simply the limiting case of weight=0. The underlying algorithm is a noise weighted Expectation Maximization (EM) PCA, which has additional benefits of implementation speed and flexibility for smoothing eigenvectors to reduce the noise contribution. We present applications of this method on simulated data and QSO spectra from the Sloan Digital Sky Survey.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Data Analysis Statistics and Probability},
  file = {/Users/tomhilder/Zotero/storage/82IQZH3G/Bailey - 2012 - Principal Component Analysis with Noisy andor Missing Data.pdf;/Users/tomhilder/Zotero/storage/6L6CZ5AD/1208.html}
}

@article{bouwmans2015,
  title = {Decomposition into {{Low-rank}} plus {{Additive Matrices}} for {{Background}}/{{Foreground Separation}}: {{A Review}} for a {{Comparative Evaluation}} with a {{Large-Scale Dataset}}},
  shorttitle = {Decomposition into {{Low-rank}} plus {{Additive Matrices}} for {{Background}}/{{Foreground Separation}}},
  author = {Bouwmans, Thierry and Sobral, Andrews and Javed, Sajid and Jung, Soon Ki and Zahzah, El-Hadi},
  year = 2015,
  month = nov,
  journal = {arXiv e-prints},
  pages = {arXiv:1511.01245},
  doi = {10.48550/arXiv.1511.01245},
  url = {https://ui.adsabs.harvard.edu/abs/2015arXiv151101245B/abstract},
  urldate = {2026-02-04},
  abstract = {Recent research on problem formulations based on decomposition into low-rank plus sparse matrices shows a suitable framework to separate moving objects from the background. The most representative problem formulation is the Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit (PCP) which decomposes a data matrix in a low-rank matrix and a sparse matrix. However, similar robust implicit or explicit decompositions can be made in the following problem formulations: Robust Non-negative Matrix Factorization (RNMF), Robust Matrix Completion (RMC), Robust Subspace Recovery (RSR), Robust Subspace Tracking (RST) and Robust Low-Rank Minimization (RLRM). The main goal of these similar problem formulations is to obtain explicitly or implicitly a decomposition into low-rank matrix plus additive matrices. In this context, this work aims to initiate a rigorous and comprehensive review of the similar problem formulations in robust subspace learning and tracking based on decomposition into low-rank plus additive matrices for testing and ranking existing algorithms for background/foreground separation. For this, we first provide a preliminary review of the recent developments in the different problem formulations which allows us to define a unified view that we called Decomposition into Low-rank plus Additive Matrices (DLAM). Then, we examine carefully each method in each robust subspace learning/tracking frameworks with their decomposition, their loss functions, their optimization problem and their solvers. Furthermore, we investigate if incremental algorithms and real-time implementations can be achieved for background/foreground separation. Finally, experimental results on a large-scale dataset called Background Models Challenge (BMC 2012) show the comparative performance of 32 different robust subspace learning/tracking methods.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/5MAG65PH/Bouwmans et al. - 2015 - Decomposition into Low-rank plus Additive Matrices for BackgroundForeground Separation A Review fo.pdf}
}

@article{cai2017,
  title = {Accelerated {{Alternating Projections}} for {{Robust Principal Component Analysis}}},
  author = {Cai, HanQin and Cai, Jian-Feng and Wei, Ke},
  year = 2017,
  month = nov,
  journal = {arXiv e-prints},
  pages = {arXiv:1711.05519},
  doi = {10.48550/arXiv.1711.05519},
  url = {https://ui.adsabs.harvard.edu/abs/2017arXiv171105519C/abstract},
  urldate = {2026-02-04},
  abstract = {We study robust PCA for the fully observed setting, which is about separating a low rank matrix \$\textbackslash boldsymbol\textbraceleft L\textbraceright\$ and a sparse matrix \$\textbackslash boldsymbol\textbraceleft S\textbraceright\$ from their sum \$\textbackslash boldsymbol\textbraceleft D\textbraceright =\textbackslash boldsymbol\textbraceleft L\textbraceright +\textbackslash boldsymbol\textbraceleft S\textbraceright\$. In this paper, a new algorithm, dubbed accelerated alternating projections, is introduced for robust PCA which significantly improves the computational efficiency of the existing alternating projections proposed in [Netrapalli, Praneeth, et al., 2014] when updating the low rank factor. The acceleration is achieved by first projecting a matrix onto some low dimensional subspace before obtaining a new estimate of the low rank matrix via truncated SVD. Exact recovery guarantee has been established which shows linear convergence of the proposed algorithm. Empirical performance evaluations establish the advantage of our algorithm over other state-of-the-art algorithms for robust PCA.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/HEZU9PYC/Cai et al. - 2017 - Accelerated Alternating Projections for Robust Principal Component Analysis.pdf}
}

@article{cai2021,
  title = {Rapid {{Robust Principal Component Analysis}}: {{CUR Accelerated Inexact Low Rank Estimation}}},
  shorttitle = {Rapid {{Robust Principal Component Analysis}}},
  author = {Cai, HanQin and Hamm, Keaton and Huang, Longxiu and Li, Jiaqi and Wang, Tao},
  year = 2021,
  journal = {IEEE Signal Processing Letters},
  volume = {28},
  pages = {116--120},
  issn = {1070-9908},
  doi = {10.1109/LSP.2020.3044130},
  url = {https://ui.adsabs.harvard.edu/abs/2021ISPL...28..116C/abstract},
  urldate = {2026-02-04},
  abstract = {Robust principal component analysis (RPCA) is a widely used tool for dimension reduction. In this work, we propose a novel non-convex algorithm, coined Iterated Robust CUR (IRCUR), for solving RPCA problems, which dramatically improves the computational efficiency in comparison with the existing algorithms. IRCUR achieves this acceleration by employing CUR decomposition when updating the low rank component, which allows us to obtain an accurate low rank approximation via only three small submatrices. Consequently, IRCUR is able to process only the small submatrices and avoid the expensive computing on full matrix through the entire algorithm. Numerical experiments establish the computational advantage of IRCUR over the state-of-art algorithms on both synthetic and real-world datasets.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/5ZN7BJ8B/Cai et al. - 2021 - Rapid Robust Principal Component Analysis CUR Accelerated Inexact Low Rank Estimation.pdf}
}

@misc{cai2021a,
  title = {Learned {{Robust PCA}}: {{A Scalable Deep Unfolding Approach}} for {{High-Dimensional Outlier Detection}}},
  shorttitle = {Learned {{Robust PCA}}},
  author = {Cai, HanQin and Liu, Jialin and Yin, Wotao},
  year = 2021,
  month = oct,
  number = {arXiv:2110.05649},
  eprint = {2110.05649},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.05649},
  url = {http://arxiv.org/abs/2110.05649},
  urldate = {2026-02-04},
  abstract = {Robust principal component analysis (RPCA) is a critical tool in modern machine learning, which detects outliers in the task of low-rank matrix reconstruction. In this paper, we propose a scalable and learnable non-convex approach for high-dimensional RPCA problems, which we call Learned Robust PCA (LRPCA). LRPCA is highly efficient, and its free parameters can be effectively learned to optimize via deep unfolding. Moreover, we extend deep unfolding from finite iterations to infinite iterations via a novel feedforward-recurrent-mixed neural network model. We establish the recovery guarantee of LRPCA under mild assumptions for RPCA. Numerical experiments show that LRPCA outperforms the state-of-the-art RPCA algorithms, such as ScaledGD and AltProj, on both synthetic datasets and real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {/Users/tomhilder/Zotero/storage/WDHB92K3/Cai et al. - 2021 - Learned Robust PCA A Scalable Deep Unfolding Approach for High-Dimensional Outlier Detection.pdf;/Users/tomhilder/Zotero/storage/BBECJJPL/2110.html}
}

@article{candes2011,
  title = {Robust Principal Component Analysis?},
  author = {Cand{\`e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  year = 2011,
  month = jun,
  journal = {J. ACM},
  volume = {58},
  number = {3},
  pages = {11:1--11:37},
  issn = {0004-5411},
  doi = {10.1145/1970392.1970395},
  url = {https://dl.acm.org/doi/10.1145/1970392.1970395},
  urldate = {2026-02-05},
  abstract = {This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the {$\ell$}1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
  file = {/Users/tomhilder/Zotero/storage/NBH5UP66/Cand√®s et al. - 2011 - Robust principal component analysis.pdf}
}

@article{centofanti2024,
  title = {Robust {{Principal Components}} by {{Casewise}} and {{Cellwise Weighting}}},
  author = {Centofanti, Fabio and Hubert, Mia and Rousseeuw, Peter J.},
  year = 2024,
  month = aug,
  journal = {arXiv e-prints},
  pages = {arXiv:2408.13596},
  doi = {10.48550/arXiv.2408.13596},
  url = {https://ui.adsabs.harvard.edu/abs/2024arXiv240813596C/abstract},
  urldate = {2026-02-05},
  abstract = {Principal component analysis (PCA) is a fundamental tool for analyzing multivariate data. Here the focus is on dimension reduction to the principal subspace, characterized by its projection matrix. The classical principal subspace can be strongly affected by the presence of outliers. Traditional robust approaches consider casewise outliers, that is, cases generated by an unspecified outlier distribution that differs from that of the clean cases. But there may also be cellwise outliers, which are suspicious entries that can occur anywhere in the data matrix. Another common issue is that some cells may be missing. This paper proposes a new robust PCA method, called cellPCA, that can simultaneously deal with casewise outliers, cellwise outliers, and missing cells. Its single objective function combines two robust loss functions, that together mitigate the effect of casewise and cellwise outliers. The objective function is minimized by an iteratively reweighted least squares (IRLS) algorithm. Residual cellmaps and enhanced outlier maps are proposed for outlier detection. The casewise and cellwise influence functions of the principal subspace are derived, and its asymptotic distribution is obtained. Extensive simulations and two real data examples illustrate the performance of cellPCA.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/SWYWQHGP/Centofanti et al. - 2024 - Robust Principal Components by Casewise and Cellwise Weighting.pdf}
}

@article{delchambre2015,
  title = {Weighted Principal Component Analysis: A Weighted Covariance Eigendecomposition Approach},
  shorttitle = {Weighted Principal Component Analysis},
  author = {Delchambre, Ludovic},
  year = 2015,
  month = feb,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {446},
  number = {4},
  eprint = {1412.4533},
  primaryclass = {astro-ph},
  pages = {3545--3555},
  issn = {1365-2966, 0035-8711},
  doi = {10.1093/mnras/stu2219},
  url = {http://arxiv.org/abs/1412.4533},
  urldate = {2026-02-04},
  abstract = {We present a new straightforward principal component analysis (PCA) method based on the diagonalization of the weighted variance-covariance matrix through two spectral decomposition methods: power iteration and Rayleigh quotient iteration. This method allows one to retrieve a given number of orthogonal principal components amongst the most meaningful ones for the case of problems with weighted and/or missing data. Principal coefficients are then retrieved by fitting principal components to the data while providing the final decomposition. Tests performed on real and simulated cases show that our method is optimal in the identification of the most significant patterns within data sets. We illustrate the usefulness of this method by assessing its quality on the extrapolation of Sloan Digital Sky Survey quasar spectra from measured wavelengths to shorter and longer wavelengths. Our new algorithm also benefits from a fast and flexible implementation.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Statistics - Methodology},
  file = {/Users/tomhilder/Zotero/storage/JNEXQEHA/Delchambre - 2015 - Weighted principal component analysis a weighted covariance eigendecomposition approach.pdf;/Users/tomhilder/Zotero/storage/BUFD5H48/1412.html}
}

@article{eckart1936,
  title = {The Approximation of One Matrix by Another of Lower Rank},
  author = {Eckart, Carl and Young, Gale},
  year = 1936,
  month = sep,
  journal = {Psychometrika},
  volume = {1},
  number = {3},
  pages = {211--218},
  issn = {1860-0980},
  doi = {10.1007/BF02288367},
  url = {https://doi.org/10.1007/BF02288367},
  urldate = {2026-02-08},
  abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
  langid = {english},
  keywords = {Canonic Form,Lower Rank,Mathematical Problem,Public Policy,Statistical Theory}
}

@misc{ganguli2026,
  title = {Dganguli/Robust-Pca},
  author = {Ganguli, Deep},
  year = 2026,
  month = feb,
  url = {https://github.com/dganguli/robust-pca},
  urldate = {2026-02-10},
  abstract = {A simple Python implementation of R-PCA},
  copyright = {MIT}
}

@article{gomezgonzalez2016,
  title = {Low-Rank plus Sparse Decomposition for Exoplanet Detection in Direct-Imaging {{ADI}} Sequences. {{The LLSG}} Algorithm},
  author = {Gomez Gonzalez, C. A. and Absil, O. and Absil, P.-A. and Van Droogenbroeck, M. and Mawet, D. and Surdej, J.},
  year = 2016,
  month = may,
  journal = {Astronomy and Astrophysics},
  volume = {589},
  pages = {A54},
  issn = {0004-6361},
  doi = {10.1051/0004-6361/201527387},
  url = {https://ui.adsabs.harvard.edu/abs/2016A&A...589A..54G/abstract},
  urldate = {2026-02-08},
  abstract = {Context. Data processing constitutes a critical component of high-contrast exoplanet imaging. Its role is almost as important as the choice of a coronagraph or a wavefront control system, and it is intertwined with the chosen observing strategy. Among the data processing techniques for angular differential imaging (ADI), the most recent is the family of principal component analysis (PCA) based algorithms. It is a widely used statistical tool developed during the first half of the past century. PCA serves, in this case, as a subspace projection technique for constructing a reference point spread function (PSF) that can be subtracted from the science data for boosting the detectability of potential companions present in the data. Unfortunately, when building this reference PSF from the science data itself, PCA comes with certain limitations such as the sensitivity of the lower dimensional orthogonal subspace to non-Gaussian noise. {$<$}BR /{$>$} Aims: Inspired by recent advances in machine learning algorithms such as robust PCA, we aim to propose a localized subspace projection technique that surpasses current PCA-based post-processing algorithms in terms of the detectability of companions at near real-time speed, a quality that will be useful for future direct imaging surveys. {$<$}BR /{$>$} Methods: We used randomized low-rank approximation methods recently proposed in the machine learning literature, coupled with entry-wise thresholding to decompose an ADI image sequence locally into low-rank, sparse, and Gaussian noise components (LLSG). This local three-term decomposition separates the starlight and the associated speckle noise from the planetary signal, which mostly remains in the sparse term. We tested the performance of our new algorithm on a long ADI sequence obtained on {$\beta$} Pictoris with VLT/NACO. {$<$}BR /{$>$} Results: Compared to a standard PCA approach, LLSG decomposition reaches a higher signal-to-noise ratio and has an overall better performance in the receiver operating characteristic space. This three-term decomposition brings a detectability boost compared to the full-frame standard PCA approach, especially in the small inner working angle region where complex speckle noise prevents PCA from discerning true companions from noise.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/CGA8K3AE/Gomez Gonzalez et al. - 2016 - Low-rank plus sparse decomposition for exoplanet detection in direct-imaging ADI sequences. The LLSG.pdf}
}

@article{guan2019,
  title = {Truncated {{Cauchy Non-negative Matrix Factorization}}},
  author = {Guan, Naiyang and Liu, Tongliang and Zhang, Yangmuzi and Tao, Dacheng and Davis, Larry S.},
  year = 2019,
  month = jan,
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {41},
  number = {1},
  eprint = {1906.00495},
  primaryclass = {cs},
  pages = {246--259},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2017.2777841},
  url = {http://arxiv.org/abs/1906.00495},
  urldate = {2026-02-04},
  abstract = {Non-negative matrix factorization (NMF) minimizes the Euclidean distance between the data matrix and its low rank approximation, and it fails when applied to corrupted data because the loss function is sensitive to outliers. In this paper, we propose a Truncated CauchyNMF loss that handle outliers by truncating large errors, and develop a Truncated CauchyNMF to robustly learn the subspace on noisy datasets contaminated by outliers. We theoretically analyze the robustness of Truncated CauchyNMF comparing with the competing models and theoretically prove that Truncated CauchyNMF has a generalization bound which converges at a rate of order \$O(\textbackslash sqrt\textbraceleft\textbraceleft\textbackslash ln n\textbraceright/\textbraceleft n\textbraceright\textbraceright )\$, where \$n\$ is the sample size. We evaluate Truncated CauchyNMF by image clustering on both simulated and real datasets. The experimental results on the datasets containing gross corruptions validate the effectiveness and robustness of Truncated CauchyNMF for learning robust subspaces.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/tomhilder/Zotero/storage/WNNFMV3N/Guan et al. - 2019 - Truncated Cauchy Non-negative Matrix Factorization.pdf;/Users/tomhilder/Zotero/storage/66N8ZHAG/1906.html}
}

@inproceedings{guyon2012,
  title = {Foreground Detection via Robust Low Rank Matrix Factorization Including Spatial Constraint with {{Iterative}} Reweighted Regression},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Pattern Recognition}} ({{ICPR2012}})},
  author = {Guyon, Charles and Bouwmans, Thierry and Zahzah, El-Hadi},
  year = 2012,
  month = nov,
  pages = {2805--2808},
  issn = {1051-4651},
  url = {https://ieeexplore.ieee.org/document/6460748},
  urldate = {2026-02-04},
  abstract = {Foreground detection is the first step in video surveillance system to detect moving objects. Robust Principal Components Analysis (RPCA) shows a nice framework to separate moving objects from the background. The background sequence is then modeled by a low rank subspace that can gradually change over time, while the moving foreground objects constitute the correlated sparse outliers. In this paper, we propose to use a low-rank matrix factorization with IRLS scheme (Iteratively reweighted least squares) and to address in the minimization process the spatial connexity of the pixels. Experimental results on the Wallflower and I2R datasets show the pertinence of the proposed approach.},
  keywords = {Matrix decomposition,Minimization,Noise,Optimized production technology,Principal component analysis,Robustness,Sparse matrices},
  file = {/Users/tomhilder/Zotero/storage/KDVTRNCG/6460748.html}
}

@article{halko2011,
  title = {Finding Structure with Randomness: {{Probabilistic}} Algorithms for Constructing Approximate Matrix Decompositions},
  author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
  year = 2011,
  journal = {SIAM Review},
  volume = {53},
  number = {2},
  eprint = {https://doi.org/10.1137/090771806},
  pages = {217--288},
  doi = {10.1137/090771806},
  url = {https://doi.org/10.1137/090771806},
  abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an \$m \texttimes n\$ matrix. (i) For a dense input matrix, randomized algorithms require \$(mn (k))\$ floating-point operations (flops) in contrast to \$ (mnk)\$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to \$(k)\$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data.}
}

@misc{he2022,
  title = {Robust {{PCA}} for {{High Dimensional Data}} Based on {{Characteristic Transformation}}},
  author = {He, Lingyu and Yang, Yanrong and Zhang, Bo},
  year = 2022,
  month = apr,
  number = {arXiv:2204.01042},
  eprint = {2204.01042},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.01042},
  url = {http://arxiv.org/abs/2204.01042},
  urldate = {2026-02-04},
  abstract = {In this paper, we propose a novel robust Principal Component Analysis (PCA) for high-dimensional data in the presence of various heterogeneities, especially the heavy-tailedness and outliers. A transformation motivated by the characteristic function is constructed to improve the robustness of the classical PCA. Besides the typical outliers, the proposed method has the unique advantage of dealing with heavy-tail-distributed data, whose covariances could be nonexistent (positively infinite, for instance). The proposed approach is also a case of kernel principal component analysis (KPCA) method and adopts the robust and non-linear properties via a bounded and non-linear kernel function. The merits of the new method are illustrated by some statistical properties including the upper bound of the excess error and the behaviors of the large eigenvalues under a spiked covariance model. In addition, we show the advantages of our method over the classical PCA by a variety of simulations. At last, we apply the new robust PCA to classify mice with different genotypes in a biological study based on their protein expression data and find that our method is more accurately on identifying abnormal mice comparing to the classical PCA.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/tomhilder/Zotero/storage/CNY7N5QA/He et al. - 2022 - Robust PCA for High Dimensional Data based on Characteristic Transformation.pdf;/Users/tomhilder/Zotero/storage/9SCCQE29/2204.html}
}

@misc{hilder2026,
  title = {{{TomHilder}}/Robusta-Hmf},
  author = {Hilder, Tom},
  year = 2026,
  month = feb,
  url = {https://github.com/TomHilder/robusta-hmf},
  urldate = {2026-02-09},
  abstract = {Robust Heteroscedastic Matrix Factorisation in JAX},
  copyright = {MIT}
}

@article{hotelling1933,
  title = {Analysis of a Complex of Statistical Variables into Principal Components},
  author = {Hotelling, H.},
  year = 1933,
  journal = {Journal of Educational Psychology},
  volume = {24},
  number = {6},
  pages = {417--441},
  publisher = {Warwick \& York},
  address = {US},
  issn = {1939-2176},
  doi = {10.1037/h0071325},
  abstract = {The problem is stated in detail, a method of analysis is derived and its geometrical meaning shown, methods of solution are illustrated and certain derivative problems are discussed. (To be concluded in October issue.) (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Statistical Analysis,Statistical Variables},
  file = {/Users/tomhilder/Zotero/storage/TP4FHCF3/doiLanding.html}
}

@misc{jax2018github,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = 2018,
  url = {http://github.com/jax-ml/jax}
}

@inproceedings{ke2005,
  title = {Robust {{L}}" {{Norm Factorization}} in the {{Presence}} of {{Outliers}} and {{Missing Data}} by {{Alternative Convex Programming}}},
  booktitle = {Proceedings of the 2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05) - {{Volume}} 1 - {{Volume}} 01},
  author = {Ke, Qifa and Kanade, Takeo},
  year = 2005,
  month = jun,
  series = {{{CVPR}} '05},
  pages = {739--746},
  publisher = {IEEE Computer Society},
  address = {USA},
  doi = {10.1109/CVPR.2005.309},
  url = {https://doi.org/10.1109/CVPR.2005.309},
  urldate = {2026-02-08},
  abstract = {Matrix factorization has many applications in computer vision. Singular Value Decomposition (SVD) is the standard algorithm for factorization. When there are outliers and missing data, which often happen in real measurements, SVD is no longer applicable. For robustness Iteratively Re-weighted Least Squares (IRLS) is often used for factorization by assigning a weight to each element in the measurements. Because it uses L norm, good initialization in IRLS is critical for success, but is non-trivial. In this paper, we formulate matrix factorization as a L norm minimization problem that is solved efficiently by alternative convex programming. Our formulation 1) is robust without requiring initial weighting, 2) handles missing data straightforwardly, and 3) provides a framework in which constraints and prior knowledge (if available) can be conveniently incorporated. In the experiments we apply our approach to factorization-based structure from motion. It is shown that our approach achieves better results than other approaches (including IRLS) on both synthetic and real data.},
  isbn = {978-0-7695-2372-9},
  file = {/Users/tomhilder/Zotero/storage/FI88BCMN/Ke and Kanade - 2005 - Robust L Norm Factorization in the Presence of Outliers and Missing Data by Alternative Convex Prog.pdf}
}

@inproceedings{lakshminarayanan2011,
  title = {Robust {{Bayesian Matrix Factorisation}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Lakshminarayanan, Balaji and Bouchard, Guillaume and Archambeau, Cedric},
  year = 2011,
  month = jun,
  pages = {425--433},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v15/lakshminarayanan11a.html},
  urldate = {2026-02-04},
  abstract = {We analyse the noise arising in collaborative filtering when formalised as a probabilistic matrix factorisation problem. We show empirically that modelling row- and column-specific variances is important, the noise being in general non-Gaussian and heteroscedastic. We also advocate for the use of a Student-t prior for the latent features as the standard Gaussian is included as a special case. We derive several variational inference algorithms and estimate the hyperparameters by type-II maximum likelihood. Experiments on real data show that the predictive performance is significantly improved.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/BUMNW2E2/Lakshminarayanan et al. - 2011 - Robust Bayesian Matrix Factorisation.pdf}
}

@article{lange1989,
  title = {Robust {{Statistical Modeling Using}} the t {{Distribution}}},
  author = {Lange, Kenneth L. and Little, Roderick J. A. and Taylor, Jeremy M. G.},
  year = 1989,
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {84},
  number = {408},
  pages = {881--896},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1989.10478852},
  url = {https://doi.org/10.1080/01621459.1989.10478852},
  urldate = {2026-02-09},
  abstract = {The t distribution provides a useful extension of the normal for statistical modeling of data sets involving errors with longer-than-normal tails. An analytical strategy based on maximum likelihood for a general model with multivariate t errors is suggested and applied to a variety of problems, including linear and nonlinear regression, robust estimation of the mean and covariance matrix with missing data, unbalanced multivariate repeated-measures data, multivariate modeling of pedigree data, and multivariate nonlinear regression. The degrees of freedom parameter of the t distribution provides a convenient dimension for achieving robust statistical inference, with moderate increases in computational complexity for many models. Estimation of precision from asymptotic theory and the bootstrap is discussed, and graphical methods for checking the appropriateness of the t distribution are presented.},
  keywords = {Bootstrap,Elliptical distributions,EM algorithm,Maximum likelihood,Nonlinear regression,Outliers,Pedigree analysis,Regression,Repeated-measures data}
}

@article{lin2010,
  title = {The {{Augmented Lagrange Multiplier Method}} for {{Exact Recovery}} of {{Corrupted Low-Rank Matrices}}},
  author = {Lin, Zhouchen and Chen, Minming and Ma, Yi},
  year = 2010,
  month = sep,
  journal = {arXiv e-prints},
  pages = {arXiv:1009.5055},
  doi = {10.48550/arXiv.1009.5055},
  url = {https://ui.adsabs.harvard.edu/abs/2010arXiv1009.5055L/abstract},
  urldate = {2026-02-04},
  abstract = {This paper proposes scalable and fast algorithms for solving the Robust PCA problem, namely recovering a low-rank matrix with an unknown fraction of its entries being arbitrarily corrupted. This problem arises in many applications, such as image processing, web data ranking, and bioinformatic data analysis. It was recently shown that under surprisingly broad conditions, the Robust PCA problem can be exactly solved via convex optimization that minimizes a combination of the nuclear norm and the \$\textbackslash ell\textasciicircum 1\$-norm . In this paper, we apply the method of augmented Lagrange multipliers (ALM) to solve this convex program. As the objective function is non-smooth, we show how to extend the classical analysis of ALM to such new objective functions and prove the optimality of the proposed algorithms and characterize their convergence rate. Empirically, the proposed new algorithms can be more than five times faster than the previous state-of-the-art algorithms for Robust PCA, such as the accelerated proximal gradient (APG) algorithm. Moreover, the new algorithms achieve higher precision, yet being less storage/memory demanding. We also show that the ALM technique can be used to solve the (related but somewhat simpler) matrix completion problem and obtain rather promising results too. We further prove the necessary and sufficient condition for the inexact ALM to converge globally. Matlab code of all algorithms discussed are available at http://perception.csl.illinois.edu/matrix-rank/home.html},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/XM48CE93/Lin et al. - 2010 - The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices.pdf}
}

@article{netrapalli2014,
  title = {Non-Convex {{Robust PCA}}},
  author = {Netrapalli, Praneeth and Niranjan, U. N. and Sanghavi, Sujay and Anandkumar, Animashree and Jain, Prateek},
  year = 2014,
  month = oct,
  journal = {arXiv e-prints},
  pages = {arXiv:1410.7660},
  doi = {10.48550/arXiv.1410.7660},
  url = {https://ui.adsabs.harvard.edu/abs/2014arXiv1410.7660N/abstract},
  urldate = {2026-02-04},
  abstract = {We propose a new method for robust PCA -- the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is \textbraceleft\textbackslash em non-convex\textbraceright{} but easy to compute. In spite of this non-convexity, we establish exact recovery of the low-rank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For an \$m \textbackslash times n\$ input matrix (\$m \textbackslash leq n)\$, our method has a running time of \$O(r\textasciicircum 2mn)\$ per iteration, and needs \$O(\textbackslash log(1/\textbackslash epsilon))\$ iterations to reach an accuracy of \$\textbackslash epsilon\$. This is close to the running time of simple PCA via the power method, which requires \$O(rmn)\$ per iteration, and \$O(\textbackslash log(1/\textbackslash epsilon))\$ iterations. In contrast, existing methods for robust PCA, which are based on convex optimization, have \$O(m\textasciicircum 2n)\$ complexity per iteration, and take \$O(1/\textbackslash epsilon)\$ iterations, i.e., exponentially more iterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/QBPVFVYZ/Netrapalli et al. - 2014 - Non-convex Robust PCA.pdf}
}

@article{pearson1901,
  title = {{{LIII}}. {{On}} Lines and Planes of Closest Fit to Systems of Points in Space},
  author = {Pearson, Karl},
  year = 1901,
  month = nov,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  publisher = {Taylor \& Francis},
  issn = {1941-5982},
  doi = {10.1080/14786440109462720},
  url = {https://doi.org/10.1080/14786440109462720},
  urldate = {2026-02-08}
}

@article{polyak2017,
  title = {Robust {{Principal Component Analysis}}: {{An IRLS Approach}} *},
  shorttitle = {Robust {{Principal Component Analysis}}},
  author = {Polyak, Boris T. and Khlebnikov, Mikhail V.},
  year = 2017,
  month = jul,
  journal = {IFAC-PapersOnLine},
  series = {20th {{IFAC World Congress}}},
  volume = {50},
  number = {1},
  pages = {2762--2767},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2017.08.585},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896317309576},
  urldate = {2026-02-05},
  abstract = {The modern problems of optimization, estimation, signal processing, and image recognition deal with data of huge dimensions. It is important to develop effective methods and algorithms for such problems. An important idea is the construction of low-dimension approximations to large-scale data. One of the most popular methods for this purpose is the principal component analysis (PCA), which is, however, sensitive to outliers. There exist numerous robust versions of PCA, relying on sparsity ideas and {$\ell$}1 techniques. The present paper offers another approach to robust PCA exploiting Huber's functions and numerical implementation based on the Iterative Reweighted Least Squares (IRLS) method.},
  keywords = {Huber's functions,method of iteratively reweighted least squares,outliers,principal component analysis,robustness},
  file = {/Users/tomhilder/Zotero/storage/J68CH7MU/Polyak and Khlebnikov - 2017 - Robust Principal Component Analysis An IRLS Approach .pdf;/Users/tomhilder/Zotero/storage/PXN3LQYA/S2405896317309576.html}
}

@inproceedings{rodriguez2013,
  title = {Fast Principal Component Pursuit via Alternating Minimization},
  booktitle = {2013 {{IEEE International Conference}} on {{Image Processing}}},
  author = {Rodr{\'i}guez, Paul and Wohlberg, Brendt},
  year = 2013,
  month = sep,
  pages = {69--73},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2013.6738015},
  url = {https://ieeexplore.ieee.org/document/6738015},
  urldate = {2026-02-05},
  abstract = {We propose a simple alternating minimization algorithm for solving a minor variation on the original Principal Component Pursuit (PCP) functional. In computational experiments in the video background modeling problem, the proposed algorithm is able to deliver a consistent sparse approximation even after the first outer loop, (taking approximately 12 seconds for a 640 \texttimes{} 480 \texttimes{} 400 color test video) which is approximately an order of magnitude faster than Inexact ALM to construct a sparse component of the same quality.},
  keywords = {Approximation algorithms,Approximation methods,Image color analysis,Minimization,Principal Component Pursuit,Robustness,Streaming media,Video Background Modeling,Video sequences},
  file = {/Users/tomhilder/Zotero/storage/E6MJ9NIP/6738015.html}
}

@article{rontogiannis2020,
  title = {Online {{Reweighted Least Squares Robust PCA}}},
  author = {Rontogiannis, Athanasios A. and Giampouras, Paris V. and Koutroumbas, Konstantinos D.},
  year = 2020,
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  pages = {1340--1344},
  issn = {1558-2361},
  doi = {10.1109/LSP.2020.3011896},
  url = {https://ieeexplore.ieee.org/document/9149633},
  urldate = {2026-02-04},
  abstract = {The letter deals with the problem known as robust principal component analysis (RPCA), that is, the decomposition of a data matrix as the sum of a low-rank matrix component and a sparse matrix component. After expressing the low-rank matrix component in factorized form, we develop a novel online RPCA algorithm that is based entirely on reweighted least squares recursions and is appropriate for sequential data processing. The proposed algorithm is fast, memory optimal and, as corroborated by indicative empirical results on simulated data and a video processing application, competitive to the state-of-the-art in terms of estimation performance.},
  keywords = {alternating minimization,iteratively reweighted least squares,Linear programming,low-rank,Matrix decomposition,Matrix factorization,Minimization,online processing,Principal component analysis,robust PCA,Robustness,Signal processing algorithms,sparse,Sparse matrices},
  file = {/Users/tomhilder/Zotero/storage/73LEFZFQ/Rontogiannis et al. - 2020 - Online Reweighted Least Squares Robust PCA.pdf;/Users/tomhilder/Zotero/storage/YHLSIYL4/9149633.html}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = 2011,
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830}
}

@inproceedings{tang2011,
  title = {Robust Principal Component Analysis Based on Low-Rank and Block-Sparse Matrix Decomposition},
  booktitle = {2011 45th {{Annual Conference}} on {{Information Sciences}} and {{Systems}}},
  author = {Tang, Gongguo and Nehorai, Arye},
  year = 2011,
  month = mar,
  pages = {1--5},
  doi = {10.1109/CISS.2011.5766144},
  url = {https://ieeexplore.ieee.org/document/5766144},
  urldate = {2026-02-04},
  abstract = {In this paper, we propose a convex program for low-rank and block-sparse matrix decomposition. Potential applications include outlier detection when certain columns of the data matrix are outliers. We design an algorithm based on the augmented Lagrange multiplier method to solve the convex program. We solve the subproblems involved in the augmented Lagrange multiplier method using the Douglas/Peaceman-Rachford (DR) monotone operator splitting method. Numerical simulations demonstrate the accuracy of our method compared with the robust principal component analysis based on low-rank and sparse matrix decomposition.},
  keywords = {Algorithm design and analysis,augmented Lagrange multiplier method,low-rank and block-sparse matrix decomposition,Matrix decomposition,Numerical simulation,operator splitting method,Optimization,Principal component analysis,robust principal component analysis,Robustness,Sparse matrices},
  file = {/Users/tomhilder/Zotero/storage/HJKR937T/5766144.html}
}

@article{tsalmantza2012,
  title = {A {{Data-driven Model}} for {{Spectra}}: {{Finding Double Redshifts}} in the {{Sloan Digital Sky Survey}}},
  shorttitle = {A {{Data-driven Model}} for {{Spectra}}},
  author = {Tsalmantza, P. and Hogg, David W.},
  year = 2012,
  month = jul,
  journal = {The Astrophysical Journal},
  volume = {753},
  number = {2},
  pages = {122},
  issn = {0004-637X},
  doi = {10.1088/0004-637X/753/2/122},
  url = {https://ui.adsabs.harvard.edu/abs/2012ApJ...753..122T/abstract},
  urldate = {2026-02-05},
  abstract = {We present a data-driven method---heteroscedastic matrix factorization, a kind of probabilistic factor analysis---for modeling or performing dimensionality reduction on observed spectra or other high-dimensional data with known but non-uniform observational uncertainties. The method uses an iterative inverse-variance-weighted least-squares minimization procedure to generate a best set of basis functions. The method is similar to principal components analysis (PCA), but with the substantial advantage that it uses measurement uncertainties in a responsible way and accounts naturally for poorly measured and missing data; it models the variance in the noise-deconvolved data space. A regularization can be applied, in the form of a smoothness prior (inspired by Gaussian processes) or a non-negative constraint, without making the method prohibitively slow. Because the method optimizes a justified scalar (related to the likelihood), the basis provides a better fit to the data in a probabilistic sense than any PCA basis. We test the method on Sloan Digital Sky Survey (SDSS) spectra, concentrating on spectra known to contain two redshift components: these are spectra of gravitational lens candidates and massive black hole binaries. We apply a hypothesis test to compare one-redshift and two-redshift models for these spectra, utilizing the data-driven model trained on a random subset of all SDSS spectra. This test confirms 129 of the 131 lens candidates in our sample and all of the known binary candidates, and turns up very few false positives.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/H22RLD8T/Tsalmantza and Hogg - 2012 - A Data-driven Model for Spectra Finding Double Redshifts in the Sloan Digital Sky Survey.pdf}
}

@article{vaswani2018,
  title = {Robust {{Subspace Learning}}: {{Robust PCA}}, Robust Subspace Tracking, and Robust Subspace Recovery},
  shorttitle = {Robust {{Subspace Learning}}},
  author = {Vaswani, Namrata and Bouwmans, Thierry and Javed, Sajid and Narayanamurthy, Praneeth},
  year = 2018,
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {4},
  pages = {32--55},
  issn = {1053-5888},
  doi = {10.1109/MSP.2018.2826566},
  url = {https://ui.adsabs.harvard.edu/abs/2018ISPM...35d..32V/abstract},
  urldate = {2026-02-04},
  abstract = {Principal component analysis (PCA) is one of the most widely used dimension reduction techniques. A related easier problem is termed subspace learning or subspace estimation. Given relatively clean data, both are easily solved via singular value decomposition (SVD).},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/W3C8I9FW/Vaswani et al. - 2018 - Robust Subspace Learning Robust PCA, robust subspace tracking, and robust subspace recovery.pdf}
}

@article{wipf2009,
  title = {A Unified {{Bayesian}} Framework for {{MEG}}/{{EEG}} Source Imaging},
  author = {Wipf, David and Nagarajan, Srikantan},
  year = 2009,
  month = feb,
  journal = {NeuroImage},
  volume = {44},
  number = {3},
  pages = {947--966},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2008.02.059},
  url = {https://www.sciencedirect.com/science/article/pii/S1053811908001870},
  urldate = {2026-02-05},
  abstract = {The ill-posed nature of the MEG (or related EEG) source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an infinite set of candidates. Bayesian approaches are useful in this capacity because they allow these assumptions to be explicitly quantified using postulated prior distributions. However, the means by which these priors are chosen, as well as the estimation and inference procedures that are subsequently adopted to affect localization, have led to a daunting array of algorithms with seemingly very different properties and assumptions. From the vantage point of a simple Gaussian scale mixture model with flexible covariance components, this paper analyzes and extends several broad categories of Bayesian inference directly applicable to source localization including empirical Bayesian approaches, standard MAP estimation, and multiple variational Bayesian (VB) approximations. Theoretical properties related to convergence, global and local minima, and localization bias are analyzed and fast algorithms are derived that improve upon existing methods. This perspective leads to explicit connections between many established algorithms and suggests natural extensions for handling unknown dipole orientations, extended source configurations, correlated sources, temporal smoothness, and computational expediency. Specific imaging methods elucidated under this paradigm include the weighted minimum {$\ell$}2-norm, FOCUSS, minimum current estimation, VESTAL, sLORETA, restricted maximum likelihood, covariance component estimation, beamforming, variational Bayes, the Laplace approximation, and automatic relevance determination, as well as many others. Perhaps surprisingly, all of these methods can be formulated as particular cases of covariance component estimation using different concave regularization terms and optimization rules, making general theoretical analyses and algorithmic extensions/improvements particularly relevant.},
  file = {/Users/tomhilder/Zotero/storage/A8SDE8BR/Wipf and Nagarajan - 2009 - A unified Bayesian framework for MEGEEG source imaging.pdf;/Users/tomhilder/Zotero/storage/ZKAHY23A/S1053811908001870.html}
}

@inproceedings{wohlberg2012,
  title = {Local Principal Component Pursuit for Nonlinear Datasets},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wohlberg, Brendt and Chartrand, Rick and Theiler, James},
  year = 2012,
  month = mar,
  pages = {3925--3928},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2012.6288776},
  url = {https://ieeexplore.ieee.org/document/6288776},
  urldate = {2026-02-04},
  abstract = {A robust version of Principal Component Analysis (PCA) can be constructed via a decomposition of a data matrix into low rank and sparse components, the former representing a low-dimensional linear model of the data, and the latter representing sparse deviations from the low-dimensional subspace. This decomposition has been shown to be highly effective, but the underlying model is not appropriate when the data are not modeled well by a single low-dimensional subspace. We construct a new decomposition corresponding to a more general underlying model consisting of a union of low-dimensional subspaces, and demonstrate the performance on a video background removal problem.},
  keywords = {Cameras,Compressive Sensing,Data models,Group Sparse,Low Rank,Manifolds,Matrix decomposition,Principal component analysis,Robust Principal Component Analysis,Robustness,Sparse matrices,Sparse Representation},
  file = {/Users/tomhilder/Zotero/storage/XX8VTQAD/6288776.html}
}

@inproceedings{wright2009,
  title = {Robust {{Principal Component Analysis}}: {{Exact Recovery}} of {{Corrupted Low-Rank Matrices}} via {{Convex Optimization}}},
  shorttitle = {Robust {{Principal Component Analysis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wright, John and Ganesh, Arvind and Rao, Shankar and Peng, Yigang and Ma, Yi},
  year = 2009,
  volume = {22},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2009/hash/c45147dee729311ef5b5c3003946c48f-Abstract.html},
  urldate = {2026-02-04},
  file = {/Users/tomhilder/Zotero/storage/8FSF4TIP/Wright et al. - 2009 - Robust Principal Component Analysis Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimi.pdf}
}

@article{yin2019,
  title = {Stable {{Principal Component Pursuit}} via {{Convex Analysis}}},
  author = {Yin, Lei and Parekh, Ankit and Selesnick, Ivan},
  year = 2019,
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {67},
  number = {10},
  pages = {2595--2607},
  issn = {1941-0476},
  doi = {10.1109/TSP.2019.2907264},
  url = {https://ieeexplore.ieee.org/document/8673650},
  urldate = {2026-02-09},
  abstract = {This paper aims to recover a low-rank matrix and a sparse matrix from their superposition observed in additive white Gaussian noise by formulating a convex optimization problem with a non-separable non-convex regularization. The proposed nonconvex penalty function extends the recent work of a multivariate generalized minimax-concave penalty for promoting sparsity. It avoids underestimation characteristic of convex regularization, which is weighted sum of nuclear norm and {$\ell$}1 norm in our case. Due to the availability of convex-preserving strategy, the cost function can be minimized through forward-backward splitting. The performance of the proposed method is illustrated for both numerical simulation and hyperspectral images restoration.},
  keywords = {AWGN,convex function,Convex functions,Convolution,Estimation,Linear programming,optimization,Optimization,Principal component analysis,Sparse matrices},
  file = {/Users/tomhilder/Zotero/storage/94U9NQJP/8673650.html}
}

@article{zhang2018a,
  title = {Heteroskedastic {{PCA}}: {{Algorithm}}, {{Optimality}}, and {{Applications}}},
  shorttitle = {Heteroskedastic {{PCA}}},
  author = {Zhang, Anru R. and Cai, T. Tony and Wu, Yihong},
  year = 2018,
  month = oct,
  journal = {arXiv e-prints},
  pages = {arXiv:1810.08316},
  doi = {10.48550/arXiv.1810.08316},
  url = {https://ui.adsabs.harvard.edu/abs/2018arXiv181008316Z/abstract},
  urldate = {2026-02-04},
  abstract = {A general framework for principal component analysis (PCA) in the presence of heteroskedastic noise is introduced. We propose an algorithm called HeteroPCA, which involves iteratively imputing the diagonal entries of the sample covariance matrix to remove estimation bias due to heteroskedasticity. This procedure is computationally efficient and provably optimal under the generalized spiked covariance model. A key technical step is a deterministic robust perturbation analysis on singular subspaces, which can be of independent interest. The effectiveness of the proposed algorithm is demonstrated in a suite of problems in high-dimensional statistics, including singular value decomposition (SVD) under heteroskedastic noise, Poisson PCA, and SVD for heteroskedastic and incomplete data.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/KW37YUHK/Zhang et al. - 2018 - Heteroskedastic PCA Algorithm, Optimality, and Applications.pdf}
}

@article{zhou2010,
  title = {Stable {{Principal Component Pursuit}}},
  author = {Zhou, Zihan and Li, Xiaodong and Wright, John and Candes, Emmanuel and Ma, Yi},
  year = 2010,
  month = jan,
  journal = {arXiv e-prints},
  pages = {arXiv:1001.2363},
  doi = {10.48550/arXiv.1001.2363},
  url = {https://ui.adsabs.harvard.edu/abs/2010arXiv1001.2363Z/abstract},
  urldate = {2026-02-09},
  abstract = {In this paper, we study the problem of recovering a low-rank matrix (the principal components) from a high-dimensional data matrix despite both small entry-wise noise and gross sparse errors. Recently, it has been shown that a convex program, named Principal Component Pursuit (PCP), can recover the low-rank matrix when the data matrix is corrupted by gross sparse errors. We further prove that the solution to a related convex program (a relaxed PCP) gives an estimate of the low-rank matrix that is simultaneously stable to small entrywise noise and robust to gross sparse errors. More precisely, our result shows that the proposed convex program recovers the low-rank matrix even though a positive fraction of its entries are arbitrarily corrupted, with an error bound proportional to the noise level. We present simulation results to support our result and demonstrate that the new convex program accurately recovers the principal components (the low-rank matrix) under quite broad conditions. To our knowledge, this is the first result that shows the classical Principal Component Analysis (PCA), optimal for small i.i.d. noise, can be made robust to gross sparse errors; or the first that shows the newly proposed PCP can be made stable to small entry-wise perturbations.},
  langid = {english},
  file = {/Users/tomhilder/Zotero/storage/UGAPCUHV/Zhou et al. - 2010 - Stable Principal Component Pursuit.pdf}
}
